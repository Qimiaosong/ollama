"""è¿™ä¸¤è¡Œæ˜¯ä» langchain_communityï¼ˆLangChain çš„ç¤¾åŒºæ‰©å±•æ¨¡å—ï¼‰
ä¸­å¯¼å…¥äº†ä¸¤ç§ PDF åŠ è½½å™¨
"""
# ç”¨äºè¯»å–æœ¬åœ° æœªç»“æ„åŒ– çš„ PDF æ–‡ä»¶å†…å®¹
from langchain_community.document_loaders import UnstructuredPDFLoader
# ç”¨äºä» çº¿ä¸Š URL åŠ è½½ PDF æ–‡ä»¶ï¼ˆè™½ç„¶è¿™é‡Œæ²¡ç”¨åˆ°ï¼‰
from langchain_community.document_loaders import OnlinePDFLoader

doc_path = "./data/BOI.pdf" # PDFæ–‡ä»¶åœ¨æœ¬åœ°çš„è·¯å¾„
model = "llama3.2" # æœ¬ä»£ç ä¸­ä½¿ç”¨çš„å¤§è¯­è¨€æ¨¡å‹

# åŠ è½½æœ¬åœ°æ–‡ä»¶
if doc_path:
    # åˆ›å»ºä¸€ä¸ªPDFåŠ è½½å™¨å¯¹è±¡
    loader = UnstructuredPDFLoader(file_path=doc_path)
    # æ‰§è¡ŒåŠ è½½ï¼Œå¾—åˆ°çš„æ˜¯ä¸€ä¸ªæ–‡æ¡£åˆ—è¡¨ï¼ˆLangChainä¸­çš„Documentå¯¹è±¡ï¼‰
    data = loader.load()
    print("done loading....")
else:
    # å¦‚æœæ²¡æœ‰æœ¬åœ°æ–‡ä»¶ï¼Œä½¿ç”¨åœ¨çº¿åŠ è½½å™¨
    print("Upload a PDF file")

# è·å–è¯¥æ–‡æ¡£ç‰‡æ®µçš„çº¯æ–‡æœ¬å†…å®¹ï¼Œä¹Ÿå°±æ˜¯ PDF ç¬¬ä¸€é¡µçš„æ–‡å­—
content = data[0].page_content
# print(content[:100])

"""
å°†pdfæ–‡æ¡£åˆ‡åˆ†æˆå°å—ï¼Œå¹¶ä½¿ç”¨æœ¬åœ°Ollamaæ¨¡å‹ç”Ÿæˆå‘é‡åµŒå…¥ï¼Œ
å°†å…¶å­˜å…¥Chromaå‘é‡æ•°æ®åº“ä¸­ï¼Œä¸ºåç»­RAGé—®ç­”ä½œå‡†å¤‡
"""
# ç”¨äºä»æœ¬åœ° Ollama æ¨¡å‹ï¼ˆå¦‚ nomic-embed-textï¼‰ä¸­ç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡ã€‚
from langchain_ollama import OllamaEmbeddings
# RecursiveCharacterTextSplitter: ä¸€ä¸ªæ™ºèƒ½çš„æ–‡æœ¬åˆ‡åˆ†å™¨ï¼Œä¿æŒè¯­ä¹‰è¿ç»­æ€§çš„åŒæ—¶å°†é•¿æ–‡æ¡£æ‹†æˆå°æ®µ
from langchain_text_splitters import RecursiveCharacterTextSplitter
# ä¸€ä¸ªè½»é‡çº§ã€æ˜“æœ¬åœ°éƒ¨ç½²çš„å‘é‡æ•°æ®åº“
from langchain_community.vectorstores import Chroma

# Split and chunk
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1200, chunk_overlap=300)
# data æ˜¯ä» PDF ä¸­æå–çš„ Document åˆ—è¡¨ï¼Œæ‰§è¡Œåˆ‡å—æ“ä½œã€‚
chunks = text_splitter.split_documents(data)
print("done splitting....")

# print(f"Number of chunks: {len(chunks)}")
# print(f"Example chunk: {chunks[0]}")

# ===== Add to vector database ===
import ollama
# ä½¿ç”¨ pull() ä¸‹è½½åµŒå…¥æ¨¡å‹ nomic-embed-textï¼Œè¿™æ˜¯ Ollama ä¸Šå¯ä»¥è·‘çš„ text embedding æ¨¡å‹
ollama.pull("nomic-embed-text")

vector_db = Chroma.from_documents(
    documents=chunks, # å‘é‡åŒ–çš„æ•°æ®æ¥æºï¼Œæ˜¯ä½ åˆ‡å¥½çš„æ–‡æœ¬ç‰‡æ®µ
    embedding=OllamaEmbeddings(model="nomic-embed-text"), # æŒ‡å®šä½¿ç”¨ nomic-embed-text æ¨¡å‹æ¥è®¡ç®—æ¯æ®µæ–‡æœ¬çš„åµŒå…¥å‘é‡
    collection_name="simple-rag", # å‘é‡æ•°æ®åº“çš„åç§°
)
# ä½ åˆ›å»ºäº†ä¸€ä¸ªæœ¬åœ°çš„ RAG-ready å‘é‡æ•°æ®åº“ï¼ˆç”¨ Chroma å®ç°ï¼‰ï¼Œå­˜æ”¾äº†ä½  PDF çš„è¯­ä¹‰å‘é‡è¡¨ç¤º
print("done adding to vector database....")

"""
è¿™æ®µä»£ç æ˜¯æ„å»º RAGï¼ˆRetrieval-Augmented Generationï¼‰
é—®ç­”ç³»ç»Ÿçš„å®Œæ•´æµç¨‹çš„**â€œæ£€ç´¢ + ç”Ÿæˆâ€éƒ¨åˆ†**ï¼Œæ ¸å¿ƒåŠŸèƒ½æ˜¯ï¼š
ğŸ’¡ç”¨æˆ·æé—® â†’ æ£€ç´¢ç›¸å…³æ–‡æ¡£ç‰‡æ®µ â†’ ç”¨å¤§è¯­è¨€æ¨¡å‹å›ç­”é—®é¢˜ã€‚
"""
# LangChain ç”¨æ¥æ„å»ºæç¤ºè¯æ¨¡ç‰ˆçš„æ¨¡å—
from langchain.prompts import ChatPromptTemplate, PromptTemplate
# ç”¨äºè§£ææ¨¡å‹è¾“å‡ºä¸ºå­—ç¬¦ä¸²
from langchain_core.output_parsers import StrOutputParser
# ç”¨æ¥è°ƒç”¨ä½ æœ¬åœ°çš„ Ollama æ¨¡å‹è¿›è¡ŒèŠå¤©
from langchain_ollama import ChatOllama
# LangChain çš„â€œåŸæ ·ä¼ é€’â€ç»„ä»¶
from langchain_core.runnables import RunnablePassthrough
# LangChain çš„ä¸€ä¸ªå¢å¼ºæ£€ç´¢å™¨ï¼Œå¯ä»¥åŸºäºä¸€ä¸ªé—®é¢˜ç”Ÿæˆå¤šä¸ªç›¸ä¼¼é—®é¢˜æ¥æ‰©å¤§å¬å›
from langchain.retrievers.multi_query import MultiQueryRetriever

"""
åŠ è½½ä½ æœ¬åœ° Ollama ç¯å¢ƒä¸­çš„æ¨¡å‹
è¿™ä¸ªæ¨¡å‹ç”¨äºä¸¤ä»¶äº‹ï¼š
ç”Ÿæˆå¤šä¸ªé—®é¢˜ï¼ˆMultiQueryRetriever ä¸­ç”¨ï¼‰
æœ€ç»ˆå›ç­”é—®é¢˜ï¼ˆRAG ä¸­ç”¨ï¼‰
"""
llm = ChatOllama(model=model)

# a simple technique to generate multiple questions from a single question and then retrieve documents
# based on those questions, getting the best of both worlds.
QUERY_PROMPT = PromptTemplate(
    input_variables=["question"],
    template="""You are an AI language model assistant. Your task is to generate five
    different versions of the given user question to retrieve relevant documents from
    a vector database. By generating multiple perspectives on the user question, your
    goal is to help the user overcome some of the limitations of the distance-based
    similarity search. Provide these alternative questions separated by newlines.
    Original question: {question}""",
)
# æŠŠä¸€ä¸ªé—®é¢˜æ‰©å±•æˆå¤šä¸ªâ€œå˜ä½“â€ï¼Œç„¶ååˆ†åˆ«å»å‘é‡æ•°æ®åº“ä¸­æ£€ç´¢ç›¸å…³å†…å®¹ï¼Œå†åˆå¹¶ç»“æœã€‚
retriever = MultiQueryRetriever.from_llm(
    vector_db.as_retriever(), llm, prompt=QUERY_PROMPT
)


# RAG prompt
template = """Answer the question based ONLY on the following context:
{context}
Question: {question}
"""
# è¿™æ˜¯ä¼ ç»™æ¨¡å‹çš„æç¤ºè¯ç»“æ„,æŒ‡ç¤ºæ¨¡å‹â€œåªèƒ½æ ¹æ®ä¸Šä¸‹æ–‡æ¥å›ç­”é—®é¢˜"
prompt = ChatPromptTemplate.from_template(template)

# æ„å»ºå®Œæ•´çš„ RAG Chain
chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)


# res = chain.invoke(input=("what is the document about?",))
# res = chain.invoke(
#     input=("what are the main points as a business owner I should be aware of?",)
# )
# è°ƒç”¨é—®ç­”é“¾
res = chain.invoke(input=("how to report BOI?",))

print(res)